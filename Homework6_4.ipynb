{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDafV4xXuFB9bZdrMhsdwM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liangliang6v6/GraphCondensation/blob/main/Homework6_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 4(100 points):\n",
        "Implement the game of tic-tac-toe (write a class that implements an agent\n",
        "playing Tic Tac Toe and learning its Q function) using the Q-learning technique (see the\n",
        "resources/links provided in class for more details). Clearly describe your evaluation metric and\n",
        "demonstrate a few runs."
      ],
      "metadata": {
        "id": "HQdCN36cDsN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "Tic-Tac-Toe can be implemented as a reinforcement learning problem using Q-learning, where an agent learns an optimal policy by interacting with the game environment. The board is represented as a state space, with each state corresponding to a unique board configuration. The action space consists of available moves (empty positions). The agent follows an ε-greedy policy, where it explores random moves occasionally but primarily selects moves based on learned Q-values. The Q-function is updated iteratively using the Bellman equation, where the agent receives rewards for winning (+1), losing (-1), drawing (+0.5), or making a neutral move (0). Over thousands of training episodes, the agent gradually improves, shifting from random play to an optimized strategy.\n",
        "\n",
        "The performance of the trained agent is evaluated based on its win rate against a random opponent. The evaluation metric includes win percentage, draw percentage, and loss percentage over multiple test games. Demonstrations of the runs involve playing against a human or a predefined opponent, where the AI chooses moves based on its learned Q-values. After training, the AI should consistently outperform a random player by making optimal decisions. For further details, Sutton and Barto’s Reinforcement Learning: An Introduction provides foundational insights into Q-learning. Additionally, OpenAI Gym and other online resources offer tutorials on implementing RL-based game agents."
      ],
      "metadata": {
        "id": "5sLmXk8ic89Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "class TicTacToeQLearning:\n",
        "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.q_table = {}\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.state = [' '] * 9\n",
        "        self.player = 'X'\n",
        "\n",
        "    def get_state(self):\n",
        "        return ''.join(self.state)\n",
        "\n",
        "    def available_actions(self):\n",
        "        return [i for i in range(9) if self.state[i] == ' ']\n",
        "\n",
        "    def make_move(self, action):\n",
        "        if self.state[action] == ' ':\n",
        "            self.state[action] = self.player\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # check status for win\n",
        "    def check_winner(self):\n",
        "        win_conditions = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
        "        for (i, j, k) in win_conditions:\n",
        "            if self.state[i] == self.state[j] == self.state[k] and self.state[i] != ' ':\n",
        "                return self.state[i]\n",
        "        if ' ' not in self.state:\n",
        "            return 'Draw'\n",
        "        return None\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = [' '] * 9\n",
        "\n",
        "    def choose_action(self):\n",
        "        state = self.get_state()\n",
        "        if random.uniform(0, 1) < self.epsilon or state not in self.q_table:\n",
        "            return random.choice(self.available_actions())  # Explore\n",
        "        else:\n",
        "            return max(self.q_table[state], key=self.q_table[state].get)  # Exploit\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = {a: 0 for a in range(9)}\n",
        "        if next_state not in self.q_table:\n",
        "            self.q_table[next_state] = {a: 0 for a in range(9)}\n",
        "\n",
        "        old_q = self.q_table[state][action]\n",
        "        max_future_q = max(self.q_table[next_state].values())\n",
        "        self.q_table[state][action] = old_q + self.alpha * (reward + self.gamma * max_future_q - old_q)\n",
        "\n",
        "    def train(self, episodes=10000):\n",
        "        for i in range(episodes):\n",
        "            if i % (episodes // 10) == 0:\n",
        "                print(f\"Training Progress: {i}/{episodes} episodes completed.\")\n",
        "            self.reset()\n",
        "            state = self.get_state()\n",
        "            while True:\n",
        "                action = self.choose_action()\n",
        "                self.make_move(action)\n",
        "                next_state = self.get_state()\n",
        "                winner = self.check_winner()\n",
        "\n",
        "                if winner == 'X':\n",
        "                    reward = 1\n",
        "                elif winner == 'O':\n",
        "                    reward = -1\n",
        "                elif winner == 'Draw':\n",
        "                    reward = 0.5\n",
        "                else:\n",
        "                    reward = 0\n",
        "\n",
        "                self.update_q_table(state, action, reward, next_state)\n",
        "                state = next_state\n",
        "\n",
        "                if winner:\n",
        "                    break\n",
        "\n",
        "\n",
        "    def play_against_human(self):\n",
        "        self.reset()\n",
        "        while True:\n",
        "            print(\"\\n\".join([\" \".join(self.state[i:i+3]) for i in range(0, 9, 3)]))\n",
        "            if self.player == 'X':\n",
        "                action = self.choose_action()\n",
        "                self.make_move(action)\n",
        "                print(f\"AI chooses position {action}\")\n",
        "            else:\n",
        "                action = int(input(\"Enter your move (0-8): \"))\n",
        "                if self.state[action] != ' ':\n",
        "                    print(\"Invalid move, try again.\")\n",
        "                    continue\n",
        "                self.make_move(action)\n",
        "\n",
        "            winner = self.check_winner()\n",
        "            if winner:\n",
        "                print(\"\\n\".join([\" \".join(self.state[i:i+3]) for i in range(0, 9, 3)]))\n",
        "                if winner == 'Draw':\n",
        "                    print(\"It's a draw!\")\n",
        "                else:\n",
        "                    print(f\"{winner} wins!\")\n",
        "                break\n",
        "\n",
        "            self.player = 'O' if self.player == 'X' else 'X'\n",
        "\n",
        "# train and test the agent\n",
        "ttt_agent = TicTacToeQLearning()\n",
        "ttt_agent.train(50000)\n",
        "\n",
        "print(\"Trained AI, now playing against human:\")\n",
        "ttt_agent.play_against_human()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gAJSJ-tc9VG",
        "outputId": "9b817239-42c8-4cdd-9ad3-095dc1ff0598"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Progress: 0/50000 episodes completed.\n",
            "Training Progress: 5000/50000 episodes completed.\n",
            "Training Progress: 10000/50000 episodes completed.\n",
            "Training Progress: 15000/50000 episodes completed.\n",
            "Training Progress: 20000/50000 episodes completed.\n",
            "Training Progress: 25000/50000 episodes completed.\n",
            "Training Progress: 30000/50000 episodes completed.\n",
            "Training Progress: 35000/50000 episodes completed.\n",
            "Training Progress: 40000/50000 episodes completed.\n",
            "Training Progress: 45000/50000 episodes completed.\n",
            "Trained AI, now playing against human:\n",
            "     \n",
            "     \n",
            "     \n",
            "AI chooses position 0\n",
            "X    \n",
            "     \n",
            "     \n",
            "Enter your move (0-8): 2\n",
            "X   O\n",
            "     \n",
            "     \n",
            "AI chooses position 4\n",
            "X   O\n",
            "  X  \n",
            "     \n",
            "Enter your move (0-8): 5\n",
            "X   O\n",
            "  X O\n",
            "     \n",
            "AI chooses position 7\n",
            "X   O\n",
            "  X O\n",
            "  X  \n",
            "Enter your move (0-8): 1\n",
            "X O O\n",
            "  X O\n",
            "  X  \n",
            "AI chooses position 3\n",
            "X O O\n",
            "X X O\n",
            "  X  \n",
            "Enter your move (0-8): 8\n",
            "X O O\n",
            "X X O\n",
            "  X O\n",
            "O wins!\n"
          ]
        }
      ]
    }
  ]
}